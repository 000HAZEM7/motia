Why This Manifesto Exists
The software industry has a habit of bolting the future onto the past. We added microservices to monolith thinking, serverless to microservice sprawl, and now we’re stapling large‑language models onto REST chains that were never designed for probabilistic, long‑running work. I’ve spent the last twelve years shipping all of those layers—loving their wins, cursing their costs, and watching each “fix” expose a fresh bottleneck.
This essay is a field report from that trench. It traces the arc that took us from Rails scaffolds to autonomous agents, shows why event‑driven architecture is the connective tissue we’ve been circling for 25 years, and explains why I built Motia to finish the job. If you’re tired of stitching LLM calls, cron jobs, and human approvals together with duct tape, read on. The backend is shifting again—let’s get ahead of it this time.
1 · I’ve Seen This Movie Twice
Back in 2013 I was building internal admin systems for decidedly “non‑tech” outfits—a screen‑printing shop here, an insurance‑claims mail‑forwarder there. None of them thought of themselves as software companies, yet my code was creeping into the core of their businesses. Whole departments were being recast as web forms, status pipelines, and background jobs. One day the operations manager for a plumbing firm told me, “We’re not a tech company.” I laughed: you’re already a software company—you just happen to know a lot about pipes.
Those early systems were essentially graphs of business workflows: each resource (Purchase Order, SKU, Claim) moved through a multi‑stage lifecycle, with employees and customers nudging it forward. Anything we could automate lived in cron jobs or RPA scripts, but most steps still relied on humans reading, reasoning, and updating records.
Fast‑forward to November 2022. ChatGPT launches on a random Thursday, and the déjà‑vu hits hard: every company is about to become an AI company, whether it likes the label or not. The same plot, only running at 10× speed. The mundane human steps I’d wired up a decade earlier now looked like low‑hanging fruit for agents and LLMs. The question wasn’t if they’d be automated—it was how soon and on what kind of backend. That realization lit the fuse for Motia.
2 · Twenty‑Five Years of Digital Transformation in Four Beats
Beat 1 — Monoliths & MVC (2000 ‑ 2009)
When Rails burst onto the scene in 2004, it didn’t just popularize Ruby—it rebooted web development around an opinionated MVC playbook. Soon every language cloned the formula: Django, Laravel, Spring MVC, ASP.NET MVC. Convention‑over‑configuration, scaffolding, testing, and a batteries‑included philosophy collapsed months of boilerplate into days of shipping. 
Win: ruthless developer velocity and a shared mental model. 
Cost: a single codebase that could only scale vertically for so long.
Beat 2 — Microservices & Containers (2010 ‑ 2015)
Containers, service discovery, and CI/CD pipelines taught us to carve monoliths into bite‑size deployables. Teams got autonomy; hotspots could scale independently. 
Win: elastic scale and fault isolation. 
Cost: an explosion of HTTP chatter and the operational overhead of stitching a hundred moving parts back into something maintainable and debuggable.
Beat 3 — Serverless & API‑First (2016 ‑ 2022)
When AWS Lambda hit general availability, “just ship a function” replaced “provision a server.” Compute became a pay‑per‑millisecond utility; every product—from Stripe to Twilio—exposed crisp REST or webhook endpoints you could call from those functions. Everything was suddenly an API, and deployment felt like flipping a light switch.
Win: zero‑ops scalability and a rich marketplace of plug‑and‑play services.
Cost: logic scattered across opaque vendor runtimes; debugging an “event storm” meant wading through ten different dashboards.
(Low‑/no‑code builders like Zapier and Make rode this same API wave, but the core shift was serverless utility compute meeting an explosion of callable services.)
Beat 4 — Agents & Event‑Native Systems (2023 ‑ ?)
Large‑language‑model agents arrived, eager to replace the human reasoning that still ties workflows together. They don’t speak HTTP, they speak probability—which means longer latencies, stochastic outcomes, and a need for real‑time evaluation. 
Win: automating the context‑heavy decisions people used to make.
Cost: orchestration complexity that traditional request/response backends were never designed to handle.
Through‑line
Each wave solved the previous bottleneck—developer speed, horizontal scale, operational friction, human‑in‑the‑loop toil—yet exposed the next constraint.
Foreshadowing
The next-generation framework will borrow the strongest traits of its predecessors: the opinionated productivity of Rails, the decoupled resilience of microservices, the elastic ops model of serverless, and the event focus demanded by agents. Something that feels familiar and future‑proof—without making us relive the same trade‑offs.
3. Businesses Are Giant Graphs of Steps
The Raw Topology
Every purchase order, ticket, invoice, or shipment moves through a chain of state‑changing steps: a status flip triggers a webhook, an approval spawns a background job, an exception fires a compensation loop. Stitch those chains together and you get a single, sprawling graph—thousands of nodes and edges that no human can fully load into working memory.
Workflows as Human‑Sized Lenses
Teams stay sane by drawing soft boundaries around step clusters—“sales pipeline,” “fulfillment,” “collections.” These workflows (and their sub‑workflows) are lenses, not walls: they exist so we can visualize slices of the graph, run focused tests, or hand an auditor a clean story. Underneath, the steps remain part of one continuous mesh.
Implications for Any Modern Framework
 A backend model built for today’s organizations has to respect both realities:
First‑class steps that connect freely across the entire graph.


Lightweight, nestable workflow overlays that teams can zoom into or collapse on demand—useful during development, optional at runtime.
Get either side wrong and you’re back to spreadsheets, tribal knowledge, and 3 a.m. Post‑its—the very complexity we’re trying to retire.
4. From Chatbots to Agents → Ambient Workflows
Since ChatGPT dropped, the industry’s mental model for “what to do with an LLM” has raced through five distinct phases—each pushing the boundary between UI trick and backend reality further to the right:

Stage (Timeline)
Market Mindset / Typical Build
Core Limitation That Prompted the Next Phase
Chatbots (Q4 2022)
Drop a textbox on your site; pipe queries to gpt‑3.5‑turbo.
Stateless Q&A — no memory, no actions
Copilot Assistants (Early 2023)
Inline AI in existing apps: draft emails, suggest code
Single‑turn tasks, limited domain context
Tool‑Calling Agents (Mid 2023)
Function‑calling to hit one API: “Schedule on Calendar.”
Can’t handle multi‑step flows or external events
Orchestrated Agents (2023‑24)
Multi‑step planners (AutoGPT, CrewAI, LangGraph); wait on webhooks, loop retries
Ops complexity, fragile retries, no audit trail
Ambient Workflows (Emerging)
Agent logic becomes an event handler: POST /generate‑report, Kafka consumer, LLM loop — all traced & scaled the same way
Boundaries dissolve — endpoint, function, and agent collapse into one Step


Key idea: 
The farther we move right, the fuzzier the distinctions become. What starts as a chatbot UI slowly melts into task‑specific assistants, then into multi‑step agents, and finally into ambient steps that are indistinguishable from any other backend handler.

When that happens, arguments about “agent frameworks” vs. “application frameworks” stop making sense—the backend itself is the framework.
5. Today’s Solution Landscape & Its Fractures






Six Camps, One Goal
Bucket
Canonical Tools
Core Super‑power
No‑Code
Zapier, Make
Fastest “hello world” automations
Low‑Code
n8n, Gumloop
Drop into code when drag‑and‑drop hits a wall
AI / ML SDKs
LangChain, LlamaIndex, DSPy
Abstract prompt + embedding plumbing
Agent Runtimes
CrewAI, Agnu
Built‑in planning loops and memory
Function‑Calling Frameworks
Mastra
Code‑first contracts & strong typing between LLM and function layer
Event‑Driven
Early EDA toolkits
Decades of queuing semantics—retries, ordering, durability


What Each Gets Right
No‑/Low‑Code → democratize small integrations and prototypes.


AI SDKs → hide the boilerplate of prompt engineering and vector search.


Agent Runtimes → give you autonomy primitives without rolling your own.


Function‑Calling → enforces explicit, typed interfaces—great developer ergonomics.


Event‑Driven EDA → proven reliability and scalability baked in.



Where the Cracks Show
Bucket
Hidden Pain Point
No‑/Low‑Code
Opaque logic and brittle UI when complexity grows.
AI / ML SDKs
Orchestration ends where ops and testing begin—DIY territory.
Agent Runtimes
Separate runtime means duplicate observability, deployment, and policy layers.
Function‑Calling
Choreographing many async or out‑of‑system events (webhooks, timers, human approvals) explodes into ad‑hoc glue code.
Event‑Driven
Most are plumbing‑only—missing the Rails‑like developer ergonomics teams still crave.

Net result: 
Every camp nails one dimension—speed, typing, autonomy, or durability—but none delivers all four in one cohesive, code‑first package. That market gap is the opening for the next generation of backend framework.
6. Old Automations vs. New Expectations
Traditional Automation & RPA Hit a Ceiling
Cron jobs, shell scripts, and screen‑scraping RPA bots were fine for repetitive, deterministic tasks. But the moment a UI changed, a record skipped, or a human needed to approve an edge case, the brittle glue snapped. Scale meant spawning more bots—each one another liability to patch and monitor.
BPM Workflows Slow to a Crawl
Business‑process‑management suites promised drag‑and‑drop flowcharts, SLAs, and audit trails. In practice they required months of modeling, heavyweight engines, and armies of “process owners.” Any tweak to real‑world logic demanded a change‑request ticket, a governance meeting, and a redeploy—hardly agile.
Agent SDKs: Smart Logic, Thin Ops
New agent frameworks handle reasoning, planning, and tool‑use elegantly, but they assume someone else provides retries, timeouts, human‑in‑loop gates, cost tracking, and versioned rollbacks. Without that operational spine, prototypes look magical while production runs feel like beta software.
The Event‑Centric, Code‑First Convergence
What teams really need is the reliability of event systems, the expressiveness of code, and the intelligence of agents—packaged in a framework opinionated enough to move fast yet porous enough to integrate anything. That convergence point doesn’t exist in legacy stacks, but it’s where modern expectations now sit.
7. Microservices Revisited—Events Eat HTTP
Integration Sprawl After the Monolith Split
Microservices cured the monolith’s scaling headaches: each team owned a deployable, scaled its hot spots, and pushed code without waiting on the entire org. But the cure came with side effects—hundreds of REST endpoints, brittle request chains, and tracing sessions that looked like forensic crime boards. As the service graph grew, integration sprawl replaced monolith bloat.
Why Events Trump REST for Agent‑Heavy Traffic
AI agents don’t align neatly with synchronous, point‑to‑point calls. They pause for LLM completions, wait on human approvals, and fire multiple downstream actions at once. An event bus handles that natively: publish‑once/fan‑out, durable retries, time‑outs, and decoupled consumers that scale independently. By shifting the “contract” from call my URL now to handle this event when you can, services stay loosely coupled and agent workflows remain resilient—even as latency, volume, and toolkits evolve. That is the architectural logic behind Motia’s choice of an event‑first core.
8. New Protocols, Same Lesson — MCP & A2A
MCP — A Universal Context Envelope
The Model Context Protocol (MCP) proposes a single, typed container for everything an AI worker might need—task prompts, user metadata, relevant docs, prior messages. Standardizing that envelope lets tool vendors and data providers plug into any model without arguing over field names.
A2A — A Common Wire for Agent Chat
Meanwhile Agent‑to‑Agent (A2A) defines how autonomous agents pass work items among themselves: intent, state, confidence scores, suggested next steps. A shared message format means a sourcing bot, a summarizer, and an executor can live on different clouds yet still collaborate fluently.
History’s Refrain — Adapt at the Edge, Stay Simple at the Core
We’ve danced this dance before: SOAP vs. REST, XML vs. JSON, AMQP vs. Kafka protocols. The successful pattern is clear: keep the core model lean—events, payload, handler—then bolt on thin adapters to speak whatever dialect the ecosystem invents next. Embed a heavyweight protocol too deeply and you’re rewriting your framework when the spec revs. A modern backend should embrace MCP and A2A at the edges, translating them into its own minimal event contract rather than rebuilding its heart around any one spec.
9. Why the Agent Pattern Will Melt Into Plumbing
Give an “agent” the ability to fire events, call downstream code, await a human approval, and ship new versions like any other function, and something interesting happens: it stops looking special. In practice it’s just another Step in the execution graph—one that happens to use an LLM for part of its logic. Strip away the marketing label and what remains is a versioned handler listening on a topic, emitting new events, and obeying the same rollout rules as the rest of the backend.
That’s the power of an event loop: it doesn’t care whether a handler runs deterministic business code, a stochastic LLM prompt chain, or a “pause‑until‑manager‑signs” gate. All three surface through the exact same contract—event in, event out, state in between. When the plumbing makes no distinction, the pattern called “agent” dissolves into implementation detail, and the entire system stays comprehensible. That unification is the abstraction at Motia’s core.

10. 25 Years of Event‑Driven Patterns—Stop Reinventing the Wheel
Queues, Pub/Sub, and Serverless Triggers
We’ve been shipping production systems on events for a quarter century. Early 2000s shops buffered traffic with RabbitMQ or JMS queues; 2010s clouds scaled that pattern into SNS, SQS, and Kafka pub/sub streams; the serverless wave finished the job, turning events into first‑class citizens via AWS Lambda, GCP Cloud Events, and every webhook‑driven SaaS in your billing sheet.
Reliability Patterns Already Solved
Out of those pipes grew battle‑tested playbooks: event sourcing for full‑replay audit trails, CQRS to separate read and write workloads, change‑data‑capture (CDC) to propagate database mutations in real time. Ordering guarantees, back‑pressure handling, at‑least‑once delivery—these aren’t research problems; they’re table‑stakes patterns every major language and cloud supports.
Agents Belong on That Backbone
So when AI agents show up needing retries, long‑running state, idempotency, and audit logs, the sane move isn’t to invent a brand‑new protocol stack—it’s to drop them onto the mature event backbone the industry already trusts. That convergence point is exactly what Motia codifies: agents as just another step in a resilient, observable, event‑driven fabric.
11. Checklist for a 2025‑Ready Backend
Massive Asynchronous Orchestration
 Thousands of long‑running, fan‑out / fan‑in steps must execute in parallel without melting the call stack—or the ops team.


Human‑in‑the‑Loop, by Design
 Approvals, overrides, expert labeling, and ad‑hoc decisions plug into the same flow model—no e‑mail hacks or side channels.


Polyglot Runtime (Multi‑Language Support)
 Steps written in TypeScript today, Python tomorrow, Go next quarter—each bundled with its own deps and deployed the same way.


Legacy & Third‑Party Connectivity
 REST, GraphQL, gRPC, SOAP, FTP, cron, on‑prem SQL—whatever still runs the business must slot into the graph with minimal glue.


Ecosystem Leverage
 Full access to the vast worlds of npm and PyPI (and future registries) so teams don’t trade today’s libraries for tomorrow’s walled gardens.


Classical Testing Patterns
 Unit tests, integration tests, mocks, and fixtures should feel as natural as they do in MVC apps—green dots in CI or it doesn’t count.


Continuous, LLM‑Centric Evaluations
 Accuracy, cost, and latency gates run automatically on every deploy; stochastic evals are first‑class, not bolt‑ons.


Nestable Workflow Hierarchies
 Collapse or expand the step graph into workflows and sub‑workflows that mirror org boundaries—zoom in to debug, zoom out to reason.
12. Meet Motia — an Event‑First, Code‑First Backend
After a decade of watching each tooling wave cure one pain and reveal another, I stopped hunting for the “perfect” framework and started building it. Motia keeps three promises drawn straight from its docs:
Opinionated like Rails – CLI scaffolds, batteries‑included defaults, and a visual Workbench.


Event‑native like Kafka & Lambda – every unit of work is a Step that publishes or subscribes to an Event.


Polyglot like today’s stacks – mix TypeScript, JavaScript, Python, and Ruby Steps inside a single Flow.


Below is the scorecard we just defined—and how Motia answers each requirement in one sentence.
2025 Checklist Item
How Motia Delivers
Massive async orchestration
The Step engine fans out across managed SQS/SNS queues; thousands of concurrent executions are durable & replay‑able.
Human‑in‑the‑loop
UI / Gateway Steps pause Flows for Slack, email, or custom Workbench prompts—no ad‑hoc side channels.
Polyglot runtime
“Plug‑and‑Play Steps in Your Language” (docs) – TS/JS & Ruby today, Python preview (≤ 250 MB) live; container‑based path tested to 10 GB bundles.
Legacy & third‑party integration
HTTP, Webhook, Cron, and On‑Prem connectors are just more Step types—same event contract, zero custom broker config.
Ecosystem leverage
Bundler ships any npm or PyPI dependency; no walled‑garden plugin store.
Classical testing
Local Runner + Jest/Pytest harness; run Steps and Flows with fixtures exactly as you would MVC controllers.
Continuous LLM eval hooks
Built‑in observability (logs, traces, Workbench inspectors) surfaces cost, latency, and output; wire automated pass/fail gates in CI.
Nestable workflow hierarchy
Core concepts Steps → Flows → Events/Topics map one‑to‑one to nested org workflows; Workbench lets you zoom from high‑level Flow to single Step.

13. Call to Builders—Join the Event‑Driven Future
Stop bending LLMs to fit decade‑old REST chains and brittle cron scripts. Wire them into a backend that speaks their language—events, concurrency, human gates, versioned rollouts—then get back to shipping product.
If that sounds like the stack you’ve been waiting for, jump in:
GitHub: https://github.com/MotiaDev/motia


Docs & Quick‑start: https://motia.dev/docs


Discord: https://discord.gg/nJFfsH5d6v


Examples: https://github.com/MotiaDev/motia-examples


NPM: https://www.npmjs.com/package/motia


Motia Workbench Extension: https://marketplace.visualstudio.com/items?itemName=motiadev.motia-vscode


MotiaCloud Alpha API keys: email mike@motia.dev or DM me on Discord
The future backend is event‑driven and agent‑native—help us finish it.

